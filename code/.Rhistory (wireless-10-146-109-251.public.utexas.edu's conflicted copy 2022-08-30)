library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
install.packages("devtools")
library(devtools)
install_github("dpuelz/BicliqueRT")
library(devtools)
install_github("dpuelz/BicliqueRT")
library(BicliqueRT)
hello
blah
install.packages("swirl")
library(swirl)
install_course_github("kosukeimai","qss-swirl")
library()
swirl()
install_course("qss-swirl")
install_course("qss-swirl")
install_course("qss-swirl")
library(swirl) # load the swirl package
install_course_github("kosukeimai", "qss-swirl")
swirl()
8-2
10^2
sqrt(9)
remotes::install_github("kosukeimai/qss-package", build_vignettes = TRUE)
test <- function(seed){
res <- seed + seed
}
test(1)
test(1)
test <- function(seed){
res <- seed + seed
}
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
##
test <- function(seed){
res <- seed + seed
res
}
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
x=c(1,2,3,4)
y=c(1,2,3,4)
table(x~y)
table(x)
summary(table)
as.table(x,y)
?Summarize
load('Downloads/tobs_debug.rds')
readRDS('Downloads/tobs_debug.rds')
readRDS('Downloads/tval_debug.rds')
readRDS('Downloads/pvalue_debug.rds')
# making sure the OLS estimates are accurate
# library(dummies)
library(RcppArmadillo)
library(Rcpp)
library(xtable)
library(bcf)
library(bayeslm)
library(glmnet)
library(dbarts)
library(rpart)
library(rpart.plot)
# load in clean and final Y, X, D
load('finaldata.RData')
setwd("~/Dropbox/Dad/FinancialWellbeing/Doh")
# making sure the OLS estimates are accurate
# library(dummies)
library(RcppArmadillo)
library(Rcpp)
library(xtable)
library(bcf)
library(bayeslm)
library(glmnet)
library(dbarts)
library(rpart)
library(rpart.plot)
# load in clean and final Y, X, D
load('finaldata.RData')
Y = as.matrix(finaldata$Y)
X = as.matrix(finaldata$X)
D = as.matrix(finaldata$D)
# Descriptive statistics
summary(Y)
sd(Y)
summary(D)
sd(D)
# just a standard linear regression: what's the coef of D
fitandprint = function(Y,X,D){
Xd = cbind(D,X)
p = dim(X)[2]
fit = lm(Y ~ Xd - 1)
beta_hat = fit$coef[2:(p+1)]
alpha_hat = fit$coef[1]
out = summary(fit)
sdOLS <- out$coefficients[1,2]
qOLS = c(alpha_hat-1.96*sdOLS,alpha_hat+1.96*sdOLS)
names(alpha_hat) = c('coefficient')
names(qOLS) = c('lower','upper')
print(round(alpha_hat,3))
print(round(qOLS,3))
}
fitandprint(Y,X,D)
# "linear regularized treatment effect"
# bayesian version of lasso/ridge regression: shrinking things toward zero (regularizing parameters)
# bayesian because have prior distribution on the vector
# shrinkage prior - horse shoe prior
fit_bayeslm = function(Y,X,D){
fittreat = bayeslm(D~X-1,prior='horseshoe',N=100000,burnin=10000) # first stage -> fitted object
beta = as.matrix(colMeans(fittreat$beta)) # -> coef from the line of best fit -> beta is the posterior mean
Dhat = X%*%beta # the prediction based off of the posterior means - fitted values from the linear regression, a function of X
DDhatX = cbind(D,Dhat,X) # D -> gives you the causal effect
p = dim(X)[2]
fitobs = bayeslm(Y = Y,X = DDhatX,prior='horseshoe',penalize = c(0,0,rep(1,p)),icept=FALSE,singular=FALSE,N=100000,burnin=10000) # second stage, the outcomee model
alpha = fitobs$beta[,1]
beta = fitobs$beta[,3:(p+2)]
betamean = colMeans(beta)
# return(list(alpha=alpha, fitobs=fitobs)) # list to return multiple things
return(fitobs) # list to return multiple things
}
# bayesian causal forests (built on ensemble ML): get the function of alpha by X
fit_bcf = function(Y,X,D,datnew){
if(typeof(D)!="logical"){ # the current version of bcf takes only binary variables
D = as.numeric((D>median(D))) # 1 if it's greater
}
fit_glmnet = cv.glmnet(x=X,y=D,family='binomial') # first stage - regress D on X - binomial cuz whether yes or no for treatment?
# getting estimates for the propensity (the estimates for the probability)
# takes input X and D - it's basically a lasso regression
# lambda is the amount of penalization that I need
# using cross validation to pick the value of lambda - the coefficient on X variables
# penalization similar to bayesian prior
pihat = predict(fit_glmnet,newx=X)
# make sure X's are in factor form, etc, instead of being viewed as continuous
fit_bcf = bcf(y=Y,z=D,x_control=X,x_moderate=X,pihat=pihat,nburn=10000,nsim=90000) # using the bcf functions - truncate 0 at the end
tau = colMeans(fit_bcf$tau)
#scratch
alldata = as.data.frame(cbind(datnew,tau))
# boxplot(tau~A8,data=alldata) # tau is moderated by A8 here!!
# boxplot(tau~J2,data=alldata) # tau is moderated by all of the G's here
#
# plot(tau,exp(datnew$logA3A),pch=19,cex=0.5)
# fit=lm(exp(datnew$logA3A)~tau)
return(list(tau=tau,alldata=alldata))
}
bcf1 <- fit_bcf(Y,X,D,datnew)
5 + 3
5 - 3
5 / 3
5 ^ 3
sqrt(4)
result <- 5 + 3
result
result
print(result)
result <- 5 - 3
result
david <- "instructor"
david
david <- "instructor and author"
david
david <- "instructor and author"
david
Result <- "5"
Result
result
typeof(Result)
typeof(result)
class(result)
Result
class(Result)
class(sqrt)
world.pop <- c(2525779, 3026003, 3691173, 4449049, 5320817, 6127700, 6916183)
world.pop
pop.first <- c(2525779, 3026003, 3691173)
pop.second <- c(4449049, 5320817, 6127700, 6916183)
pop.all <- c(pop.first, pop.second)
pop.all <- c(pop.first, pop.second)
pop.first <- c(2525779, 3026003, 3691173)
pop.first
pop.all
world.pop
world.pop[2]
world.pop[c(2, 4)]
world.pop[-3]
world.pop / 1000
# We can also convert the vector to different units using mathematical operations
pop.million <- world.pop / 1000
pop.million
pop.rate <- world.pop / world.pop[1]
pop.rate
pop.increase <- world.pop[-1] - world.pop[-7]
percent.increase <- (pop.increase / world.pop[-7]) * 100
percent.increase
percent.increase[c(1, 2)] <- c(20, 22)
percent.increase
my.summary <- function(x){ # function takes one input
s.out <- sum(x)
l.out <- length(x)
m.out <- s.out / l.out
out <- c(s.out, l.out, m.out) # define the output
names(out) <- c("sum", "length", "mean") # add labels
return(out) # end function by calling output
}
my.summary
z <- 1:10
z
my.summary(z)
my.summary(world.pop)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
## ------------------------------------------------------------------------
# Let's consider an experiment where researchers have a set of resumes, and then change the names at the top to white and black sounding names.  There are 36 different names in total.  4870 total observations,
resume <- read.csv("../data/resume.csv")
setwd("~/Dropbox/SalemCenter/classes/PRL/Fall2022/Policy-Research-Laboratory/code")
## ------------------------------------------------------------------------
# Let's consider an experiment where researchers have a set of resumes, and then change the names at the top to white and black sounding names.  There are 36 different names in total.  4870 total observations,
resume <- read.csv("../data/resume.csv")
dim(resume)
head(resume)
summary(resume)
race.call.tab <- table(race = resume$race, call = resume$call)
race.call.tab
addmargins(race.call.tab)
## overall callback rate: total callbacks divided by the sample size
sum(race.call.tab[, 2]) / nrow(resume)
## callback rates for each race
race.call.tab[1, 2] / sum(race.call.tab[1, ]) # black
race.call.tab[2, 2] / sum(race.call.tab[2, ]) # white
race.call.tab[1, ]  # the first row
race.call.tab[ , 2]  # the second column
mean(resume$call) # same as line 15!
## ------------------------------------------------------------------------
## callback rate for black-sounding names
mean(resume$call[resume$race == "black"])
## race of first 5 observations
resume$race[1:5]
## comparison of first 5 observations
(resume$race == "black")[1:5]
dim(resume) # dimension of original data frame
## subset blacks only
resumeB <- resume[resume$race == "black", ]
dim(resumeB) # this data.frame has fewer rows than the original data.frame
mean(resumeB$call) # callback rate for blacks
## subset whites only
resumeW <- resume[resume$race == "white", ]
dim(resumeW) # this data.frame has fewer rows than the original data.frame
mean(resumeW$call) # callback rate for blacks
#' In this script, we investigate a policing experiment and see how to use the
#' FRT to characterize statistical uncertainty in the DiM estimator of the SATE
# First, load in all of the data
load('../data/Z_police.RData') # treatment assignments
load('../data/network_police.RData') # network
load('../data/outcomes_police.RData') # outcomes of interest
