\documentclass[11pt]{article}

% Preamble!!
\usepackage{titling}
\setlength{\voffset}{0.7in}
\setlength{\droptitle}{-10em}
\usepackage{titlesec}
\titlelabel{\thetitle.\quad}
\usepackage{xcolor}
\usepackage{adjustbox}
\usepackage{amsmath, amsthm, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{longtable}
\usepackage{breqn}
\usepackage{lscape}
\usepackage{indentfirst}
\usepackage[labelsep=period,justification=justified,singlelinecheck=false,font = footnotesize, labelfont=bf]{caption}
\usepackage{booktabs}
\usepackage{tabularx,ragged2e}
\usepackage{natbib}
\usepackage{rotating}
\usepackage{placeins}
\usepackage{subcaption}
\usepackage{hyperref}
\definecolor{burntorange}{rgb}{0.8, 0.33, 0.0}
\hypersetup{
    colorlinks=true,
    linkcolor=orange,
    filecolor=magenta,      
    urlcolor=burntorange,
}
\pagenumbering{arabic}

\usepackage{bbm}
\usepackage[margin=1in]{geometry}

\usepackage{enumerate}
\usepackage{array}
\usepackage[T1]{fontenc}
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
\usepackage{mathtools}
\newcommand\eho{\stackrel{\mathclap{\small\mbox{$H_0$}}}{=}}
\newcommand\sho{\stackrel{\mathclap{\small\mbox{$*$}}}{=}}
\newcommand\dho{\stackrel{\mathclap{\small\mbox{$d$}}}{=}}
\newcommand\qho{\stackrel{\mathclap{\small\mbox{$?$}}}{=}}

% \usepackage{authblk}


\DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}

\usepackage{fullpage, amsmath, amssymb, amsthm, bbm, color}
\usepackage{graphicx,caption,subcaption,placeins}
\usepackage{dsfont}

\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{,}{,}

\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{procedure}{Procedure}


\usepackage{tikz}
\usetikzlibrary{positioning,chains,fit,shapes,calc}

\definecolor{myblue}{RGB}{80,80,160}
\definecolor{mygreen}{RGB}{80,160,80}


\begin{document}

\title{FIN 373 Homework 11 \\ {\large due: \textbf{11/16/21}}}
\date{}
\maketitle

\vspace{-20mm}

\noindent Instructions: Please submit solutions on canvas.  Only a knitted pdf of an {\tt Rmarkdown} file will be accepted.
\\

\noindent \textbf{Problem 1:} Text analysis gives researchers a powerful set of tools for extracting information from a large body of documents.\footnote{This exercise is based on Gentzkow, M. and Shapiro, J. M. -- \href{http://dx.doi.org/10.3982/ECTA7195}{What Drives Media Slant? Evidence From U.S. Daily Newspapers. \textit{Econometrica}. 2010. 78(1): 35-71.}}

We will analyze data from newspapers across the country to see what topics they cover and how those topics are related to their ideological bias. The authors computed a measure of a newspaper's ``slant'' by comparing its language to speeches made by Democrats and Republicans in the U.S. Congress. 

You will use three data sources for this analysis. The first, {\tt dtm}, is a document term matrix with one row per newspaper, containing the 1000 phrases -- stemmed and processed -- that do the best job of identifying the speaker as a Republican or a Democrat. For example, ``living in poverty'' is a phrase most frequently spoken by Democrats, while ``global war on terror'' is a phrase most frequently spoken by Republicans; a phrase like ``exchange rate'' would not be included in this dataset, as it is used often by members of both parties and is thus a poor indicator of ideology. 

The second object, {\tt papers}, contains some data on the newspapers on which {\tt dtm} is based. The row names in {\tt dtm} correspond to the {\tt newsid} variable in {\tt papers}. The variables are:
\vspace{1mm}
\begin{center}
\begin{tabular}{l p{10cm}}
 \hline
\textit{Variable} & \textit{Description} \\
\hline
{\tt newsid}   &                   The newspaper ID \\
{\tt paper} &                      The newspaper name\\
{\tt city} &                       The city in which the newspaper is based\\
{\tt state} &                      The state in which the newspaper is based\\
{\tt district} &                   Congressional district where the newspaper is based (data for Texas only)\\
{\tt nslant} &                     The ``ideological slant'' (lower numbers mean more Democratic) \\
\hline
\end{tabular}
\end{center}

The third object, {\tt cong}, contains data on members of Congress based on their political speech, which we will compare to the ideological slant of newspapers from the areas that these legislators represent. The variables are: 
\vspace{1mm}
\begin{center}
\begin{tabular}{l p{10cm}}
 \hline
\textit{Variable} & \textit{Description} \\
\hline
{\tt legname} &                    Legislator's name\\
{\tt state} &                      Legislator's state\\
{\tt district} &                   Legislator's Congressional district \\
{\tt chamber} &                    Chamber in which legislator serves (House or Senate)\\
{\tt party} &                      Legislator's party\\
{\tt cslant} &                     Ideological slant based on legislator's speech (lower numbers mean more Democratic) \\
 \hline
\end{tabular}
\end{center}


\begin{enumerate}[a.]
\item We will first focus on the slant of newspapers, which the authors define as the tendency to use language that would sway readers to the political left or right. Load the data and plot the distribution of {\tt nslant} in the {\tt papers} data frame, with a vertical line at the median. Which newspaper in the country has the largest left-wing slant? What about right? 
\item We will explore the content of these newspapers using the {\tt wordcloud} package. 

First load the {\tt wordcloud} package. Make a word cloud of the top words (at most 20) in the {\tt dtm} object. What were the biggest topics in the news in 2005 when these data were collected? Hint: first convert {\tt dtm} into a {\tt matrix}.

Now subset the data to the tenth of newspapers with the leftmost (lowest) political slant and the rightmost (highest) political slant. Make two word clouds showing the words most commonly used by each group of newspapers (again, at most 20 words). How does their language differ? Do they have anything in common? 
Hint: to use your usual subsetting/indexing tools, convert your {\tt dtm} matrix into a data frame using the {\tt data.frame} function. 

Pay close attention to your warnings, as they contain important information. For extra credit, see if you can make them go away. 
\item We will now explore the relationship between the political slant of newspapers and the language used by members of Congress. 

Using the dataset {\tt cong}, compute average slant by state separately for the House and Senate. Now use {\tt papers} to compute the average newspaper slant by state. Make two plots with Congressional slant on the x-axis and newspaper slant on the y-axis -- one for the House, one for the Senate. Include a best-fit line in each plot -- a red one for the Senate and a green one for the House. Label your axes, title your plots, and make sure the axes are the same for comparability. Can you conclude that newspapers are influenced by the political language of elected officials? How else can you interpret the results? 
\item We will now take a closer look at the relationship between congressional and media slant at the district level, for one particular state -- Texas. To do so, subset the two datasets to Texas alone, then merge them by district and state, keeping only the observations that appear in both datasets. Then, produce the same plot as in question 3 above, but at the district level (just for the House). What do you find?  Which results do you think are more informative, and why?
\item Identify the most important terms for capturing regional variation in what is considered newsworthy -- the terms that appear frequently in some documents, but not across all documents. To do so, compute the \textbf{term frequency-inverse document frequency (TF-IDF)} for each phrase and newspaper combination in the dataset (for this, use the {\tt tm} package and the {\tt dtm} object originally provided). 

Subset the TF-IDF transformed matrix you created to contain the newspaper closest to Princeton, the ``Home News Tribune'' of East Brunswick, NJ. Print the terms with the largest TF-IDF in decreasing order. What topics are of interest to our region, but not likely to make the national news? 
\end{enumerate}







\end{document}

